{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aee2a1d4",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Multi-Modal Violence Detection\n",
    "## CNN + BiLSTM with Pose Detection and Emotion Recognition\n",
    "\n",
    "**Expected Performance:** 92-97% accuracy on RWF-2000 dataset\n",
    "\n",
    "**âš¡ WORKFLOW:**\n",
    "1. **Add Dataset**: Add `vulamnguyen/rwf2000` dataset to notebook\n",
    "2. **Enable GPU**: Settings â†’ Accelerator â†’ GPU T4 x2\n",
    "3. **Run All**: Cell â†’ Run All\n",
    "4. **Wait 4-6 hours**: Fully automated!\n",
    "5. **Download Model**: Auto-saved to `/kaggle/working/`\n",
    "\n",
    "**Total Time:** ~4-6 hours with GPU\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36299fd2",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Section 1: Setup GPU and Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6046ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ–¥ï¸  SYSTEM INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(f\"GPU Devices: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e532e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"\\nðŸ“¦ Installing required packages...\\n\")\n",
    "\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q mediapipe\n",
    "!pip install -q deepface\n",
    "!pip install -q scikit-learn matplotlib seaborn tqdm\n",
    "\n",
    "print(\"\\nâœ… All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fc4933",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Section 2: Locate Dataset (Kaggle Auto-Mounted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dbd7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ“‚ LOCATING RWF-2000 DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Kaggle datasets are in /kaggle/input/\n",
    "possible_paths = [\n",
    "    '/kaggle/input/rwf2000/RWF-2000',\n",
    "    '/kaggle/input/rwf2000',\n",
    "    '/kaggle/input/rwf-2000/RWF-2000',\n",
    "    '/kaggle/input/rwf-2000',\n",
    "    '/kaggle/input/RWF-2000',\n",
    "]\n",
    "\n",
    "DATASET_PATH = None\n",
    "\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        check_train = os.path.join(path, 'train')\n",
    "        check_val = os.path.join(path, 'val')\n",
    "        if os.path.exists(check_train) and os.path.exists(check_val):\n",
    "            DATASET_PATH = path\n",
    "            break\n",
    "\n",
    "# If not found, search entire /kaggle/input/\n",
    "if DATASET_PATH is None:\n",
    "    print(\"ðŸ” Searching /kaggle/input/ directory...\")\n",
    "    for root, dirs, files in os.walk('/kaggle/input'):\n",
    "        if 'train' in dirs and 'val' in dirs:\n",
    "            train_path = os.path.join(root, 'train')\n",
    "            if os.path.exists(os.path.join(train_path, 'Fight')):\n",
    "                DATASET_PATH = root\n",
    "                break\n",
    "\n",
    "if DATASET_PATH is None:\n",
    "    print(\"\\nâŒ ERROR: RWF-2000 dataset not found!\")\n",
    "    print(\"\\nðŸ’¡ Please add the dataset:\")\n",
    "    print(\"   1. Click 'Add Data' button (right sidebar)\")\n",
    "    print(\"   2. Search for 'rwf2000' or 'vulamnguyen/rwf2000'\")\n",
    "    print(\"   3. Click 'Add' button\")\n",
    "    print(\"   4. Re-run this cell\")\n",
    "    print(\"\\nðŸ“ Available datasets:\")\n",
    "    !ls -la /kaggle/input/\n",
    "    raise FileNotFoundError(\"RWF-2000 dataset not found in /kaggle/input/\")\n",
    "\n",
    "print(f\"\\nâœ… Dataset found at: {DATASET_PATH}\")\n",
    "\n",
    "# Setup paths\n",
    "CACHE_PATH = '/kaggle/working/violence_detection_cache'\n",
    "MODEL_SAVE_PATH = '/kaggle/working/violence_detection_models'\n",
    "\n",
    "os.makedirs(CACHE_PATH, exist_ok=True)\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"âœ… Cache directory: {CACHE_PATH}\")\n",
    "print(f\"âœ… Models directory: {MODEL_SAVE_PATH}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d686e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š VERIFYING DATASET STRUCTURE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_fight = os.path.join(DATASET_PATH, 'train', 'Fight')\n",
    "train_nonfight = os.path.join(DATASET_PATH, 'train', 'NonFight')\n",
    "val_fight = os.path.join(DATASET_PATH, 'val', 'Fight')\n",
    "val_nonfight = os.path.join(DATASET_PATH, 'val', 'NonFight')\n",
    "\n",
    "paths = {\n",
    "    'Train/Fight': train_fight,\n",
    "    'Train/NonFight': train_nonfight,\n",
    "    'Val/Fight': val_fight,\n",
    "    'Val/NonFight': val_nonfight\n",
    "}\n",
    "\n",
    "total_videos = 0\n",
    "for name, path in paths.items():\n",
    "    if os.path.exists(path):\n",
    "        videos = [f for f in os.listdir(path) if f.endswith(('.avi', '.mp4'))]\n",
    "        count = len(videos)\n",
    "        total_videos += count\n",
    "        print(f\"âœ… {name}: {count} videos\")\n",
    "    else:\n",
    "        print(f\"âŒ {name}: NOT FOUND!\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Total videos: {total_videos}\")\n",
    "if total_videos == 2000:\n",
    "    print(\"âœ… Dataset structure is CORRECT!\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Expected 2000 videos, but found {total_videos}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c3c3a",
   "metadata": {},
   "source": [
    "## ðŸ“š Section 3: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350279ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# Pose and Emotion Detection\n",
    "import mediapipe as mp\n",
    "from deepface import DeepFace\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43caaec7",
   "metadata": {},
   "source": [
    "## âš™ï¸ Section 4: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7739304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class\n",
    "class Config:\n",
    "    # Paths\n",
    "    DATASET_DIR = DATASET_PATH\n",
    "    CACHE_DIR = CACHE_PATH\n",
    "    MODEL_SAVE_DIR = MODEL_SAVE_PATH\n",
    "    \n",
    "    # Video processing\n",
    "    SEQUENCE_LENGTH = 20\n",
    "    IMG_HEIGHT = 224\n",
    "    IMG_WIDTH = 224\n",
    "    IMG_CHANNELS = 3\n",
    "    \n",
    "    # Features\n",
    "    POSE_DIM = 120\n",
    "    EMOTION_DIM = 8\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 30\n",
    "    LEARNING_RATE = 1e-4\n",
    "    \n",
    "    # Model\n",
    "    LSTM_UNITS = 256\n",
    "    FUSION_TYPE = 'adaptive'\n",
    "    \n",
    "    # Callbacks\n",
    "    PATIENCE_EARLY_STOP = 8\n",
    "    PATIENCE_REDUCE_LR = 4\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ“Š CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Sequence Length: {config.SEQUENCE_LENGTH} frames\")\n",
    "print(f\"Image Size: {config.IMG_HEIGHT}x{config.IMG_WIDTH}\")\n",
    "print(f\"Batch Size: {config.BATCH_SIZE}\")\n",
    "print(f\"Epochs: {config.EPOCHS}\")\n",
    "print(f\"Learning Rate: {config.LEARNING_RATE}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90141b9",
   "metadata": {},
   "source": [
    "## ðŸŽ¬ Section 5: Pose and Emotion Detection Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2965e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pose and Emotion Preprocessor Class\n",
    "class PoseEmotionPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.pose_detector = self.mp_pose.Pose(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=1,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "    \n",
    "    def _calculate_angle(self, a, b, c):\n",
    "        a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "        radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "        angle = np.abs(radians*180.0/np.pi)\n",
    "        if angle > 180.0:\n",
    "            angle = 360 - angle\n",
    "        return angle\n",
    "    \n",
    "    def extract_pose_features_advanced(self, frame):\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = self.pose_detector.process(frame_rgb)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Basic keypoints\n",
    "            keypoints = []\n",
    "            for lm in landmarks:\n",
    "                keypoints.extend([lm.x, lm.y, lm.visibility])\n",
    "            \n",
    "            # Joint angles\n",
    "            left_elbow = self._calculate_angle(\n",
    "                [landmarks[11].x, landmarks[11].y],\n",
    "                [landmarks[13].x, landmarks[13].y],\n",
    "                [landmarks[15].x, landmarks[15].y]\n",
    "            )\n",
    "            right_elbow = self._calculate_angle(\n",
    "                [landmarks[12].x, landmarks[12].y],\n",
    "                [landmarks[14].x, landmarks[14].y],\n",
    "                [landmarks[16].x, landmarks[16].y]\n",
    "            )\n",
    "            left_knee = self._calculate_angle(\n",
    "                [landmarks[23].x, landmarks[23].y],\n",
    "                [landmarks[25].x, landmarks[25].y],\n",
    "                [landmarks[27].x, landmarks[27].y]\n",
    "            )\n",
    "            right_knee = self._calculate_angle(\n",
    "                [landmarks[24].x, landmarks[24].y],\n",
    "                [landmarks[26].x, landmarks[26].y],\n",
    "                [landmarks[28].x, landmarks[28].y]\n",
    "            )\n",
    "            left_shoulder = self._calculate_angle(\n",
    "                [landmarks[13].x, landmarks[13].y],\n",
    "                [landmarks[11].x, landmarks[11].y],\n",
    "                [landmarks[23].x, landmarks[23].y]\n",
    "            )\n",
    "            right_shoulder = self._calculate_angle(\n",
    "                [landmarks[14].x, landmarks[14].y],\n",
    "                [landmarks[12].x, landmarks[12].y],\n",
    "                [landmarks[24].x, landmarks[24].y]\n",
    "            )\n",
    "            \n",
    "            # Body metrics\n",
    "            hand_distance = np.sqrt(\n",
    "                (landmarks[15].x - landmarks[16].x)**2 +\n",
    "                (landmarks[15].y - landmarks[16].y)**2\n",
    "            )\n",
    "            foot_elevation = abs(landmarks[27].y - landmarks[28].y)\n",
    "            torso_bend_left = abs(landmarks[11].y - landmarks[23].y)\n",
    "            torso_bend_right = abs(landmarks[12].y - landmarks[24].y)\n",
    "            center_x = np.mean([landmarks[11].x, landmarks[12].x, landmarks[23].x, landmarks[24].x])\n",
    "            center_y = np.mean([landmarks[11].y, landmarks[12].y, landmarks[23].y, landmarks[24].y])\n",
    "            head_offset_x = landmarks[0].x - center_x\n",
    "            head_offset_y = landmarks[0].y - center_y\n",
    "            \n",
    "            advanced_features = keypoints + [\n",
    "                left_elbow, right_elbow, left_knee, right_knee,\n",
    "                left_shoulder, right_shoulder, hand_distance, foot_elevation,\n",
    "                torso_bend_left, torso_bend_right, center_x, center_y,\n",
    "                head_offset_x, head_offset_y,\n",
    "                left_elbow/180, right_elbow/180, left_knee/180, right_knee/180,\n",
    "                left_shoulder/180, right_shoulder/180, hand_distance * 10\n",
    "            ]\n",
    "            \n",
    "            return np.array(advanced_features[:120])\n",
    "        else:\n",
    "            return np.zeros(120)\n",
    "    \n",
    "    def extract_emotion_features(self, frame):\n",
    "        try:\n",
    "            analysis = DeepFace.analyze(\n",
    "                frame,\n",
    "                actions=['emotion'],\n",
    "                enforce_detection=False,\n",
    "                silent=True\n",
    "            )\n",
    "            \n",
    "            if isinstance(analysis, list):\n",
    "                analysis = analysis[0]\n",
    "            \n",
    "            emotions = analysis['emotion']\n",
    "            emotion_vector = [\n",
    "                emotions.get('angry', 0.0) / 100.0,\n",
    "                emotions.get('disgust', 0.0) / 100.0,\n",
    "                emotions.get('fear', 0.0) / 100.0,\n",
    "                emotions.get('happy', 0.0) / 100.0,\n",
    "                emotions.get('sad', 0.0) / 100.0,\n",
    "                emotions.get('surprise', 0.0) / 100.0,\n",
    "                emotions.get('neutral', 0.0) / 100.0\n",
    "            ]\n",
    "            variance = np.var(emotion_vector)\n",
    "            return np.array(emotion_vector + [variance])\n",
    "        except:\n",
    "            return np.zeros(8)\n",
    "    \n",
    "    def extract_enhanced_features(self, video_path, num_frames=20, target_size=(224, 224)):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total_frames == 0:\n",
    "            cap.release()\n",
    "            raise ValueError(f\"No frames in video: {video_path}\")\n",
    "        \n",
    "        if total_frames >= num_frames:\n",
    "            indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "        else:\n",
    "            indices = list(range(total_frames)) + [total_frames - 1] * (num_frames - total_frames)\n",
    "        \n",
    "        frames = []\n",
    "        poses = []\n",
    "        emotions = []\n",
    "        \n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if ret:\n",
    "                frame_resized = cv2.resize(frame, target_size)\n",
    "                frames.append(frame_resized)\n",
    "                poses.append(self.extract_pose_features_advanced(frame))\n",
    "                emotions.append(self.extract_emotion_features(frame))\n",
    "            else:\n",
    "                if len(frames) > 0:\n",
    "                    frames.append(frames[-1])\n",
    "                    poses.append(poses[-1])\n",
    "                    emotions.append(emotions[-1])\n",
    "                else:\n",
    "                    frames.append(np.zeros((*target_size, 3), dtype=np.uint8))\n",
    "                    poses.append(np.zeros(120))\n",
    "                    emotions.append(np.zeros(8))\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        return {\n",
    "            'frames': np.array(frames),\n",
    "            'pose': np.array(poses),\n",
    "            'emotion': np.array(emotions)\n",
    "        }\n",
    "\n",
    "print(\"âœ… PoseEmotionPreprocessor class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee655c4",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Section 6: Load Dataset Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9db5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_paths(dataset_dir):\n",
    "    train_fight_dir = os.path.join(dataset_dir, 'train', 'Fight')\n",
    "    train_nonfight_dir = os.path.join(dataset_dir, 'train', 'NonFight')\n",
    "    val_fight_dir = os.path.join(dataset_dir, 'val', 'Fight')\n",
    "    val_nonfight_dir = os.path.join(dataset_dir, 'val', 'NonFight')\n",
    "    \n",
    "    train_fight = sorted([os.path.join(train_fight_dir, f) for f in os.listdir(train_fight_dir) if f.endswith(('.avi', '.mp4'))])\n",
    "    train_nonfight = sorted([os.path.join(train_nonfight_dir, f) for f in os.listdir(train_nonfight_dir) if f.endswith(('.avi', '.mp4'))])\n",
    "    val_fight = sorted([os.path.join(val_fight_dir, f) for f in os.listdir(val_fight_dir) if f.endswith(('.avi', '.mp4'))])\n",
    "    val_nonfight = sorted([os.path.join(val_nonfight_dir, f) for f in os.listdir(val_nonfight_dir) if f.endswith(('.avi', '.mp4'))])\n",
    "    \n",
    "    train_paths = train_fight + train_nonfight\n",
    "    train_labels = [1] * len(train_fight) + [0] * len(train_nonfight)\n",
    "    \n",
    "    val_paths = val_fight + val_nonfight\n",
    "    val_labels = [1] * len(val_fight) + [0] * len(val_nonfight)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"DATASET LOADED\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Training: {len(train_paths)} videos ({len(train_fight)} Fight, {len(train_nonfight)} Non-Fight)\")\n",
    "    print(f\"Validation: {len(val_paths)} videos ({len(val_fight)} Fight, {len(val_nonfight)} Non-Fight)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return train_paths, train_labels, val_paths, val_labels\n",
    "\n",
    "train_paths, train_labels, val_paths, val_labels = load_dataset_paths(config.DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a61d0c3",
   "metadata": {},
   "source": [
    "## âš¡ Section 7: Preprocess and Cache Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc86af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_cache_dataset(video_paths, labels, cache_prefix, preprocessor):\n",
    "    cache_file = os.path.join(config.CACHE_DIR, f'{cache_prefix}_features.npz')\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"âœ… Loading cached features from: {cache_file}\")\n",
    "        data = np.load(cache_file)\n",
    "        return data['frames'], data['pose'], data['emotion'], data['labels']\n",
    "    \n",
    "    print(f\"\\nðŸ”„ Processing {len(video_paths)} videos (2-3 hours)...\")\n",
    "    print(f\"   Progress will be saved to: {cache_file}\")\n",
    "    \n",
    "    all_frames = []\n",
    "    all_poses = []\n",
    "    all_emotions = []\n",
    "    all_labels = []\n",
    "    failed = 0\n",
    "    \n",
    "    for i, (video_path, label) in enumerate(tqdm(zip(video_paths, labels), total=len(video_paths), desc=\"Extracting features\")):\n",
    "        try:\n",
    "            features = preprocessor.extract_enhanced_features(\n",
    "                video_path,\n",
    "                num_frames=config.SEQUENCE_LENGTH,\n",
    "                target_size=(config.IMG_HEIGHT, config.IMG_WIDTH)\n",
    "            )\n",
    "            \n",
    "            if features is not None:\n",
    "                all_frames.append(features['frames'].astype(np.float32) / 255.0)\n",
    "                all_poses.append(features['pose'].astype(np.float32))\n",
    "                all_emotions.append(features['emotion'].astype(np.float32))\n",
    "                all_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâš ï¸ Error processing {os.path.basename(video_path)}: {str(e)}\")\n",
    "            failed += 1\n",
    "    \n",
    "    print(f\"\\nâœ… Processed {len(all_frames)} videos successfully\")\n",
    "    if failed > 0:\n",
    "        print(f\"âš ï¸ Failed: {failed} videos\")\n",
    "    \n",
    "    all_frames = np.array(all_frames, dtype=np.float32)\n",
    "    all_poses = np.array(all_poses, dtype=np.float32)\n",
    "    all_emotions = np.array(all_emotions, dtype=np.float32)\n",
    "    all_labels = np.array(all_labels, dtype=np.float32)\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ Saving features to cache...\")\n",
    "    np.savez_compressed(\n",
    "        cache_file,\n",
    "        frames=all_frames,\n",
    "        pose=all_poses,\n",
    "        emotion=all_emotions,\n",
    "        labels=all_labels\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Features cached!\")\n",
    "    print(f\"   Frames: {all_frames.shape}\")\n",
    "    print(f\"   Pose: {all_poses.shape}\")\n",
    "    print(f\"   Emotion: {all_emotions.shape}\")\n",
    "    \n",
    "    return all_frames, all_poses, all_emotions, all_labels\n",
    "\n",
    "print(\"âœ… Preprocessing function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03785d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor and process datasets\n",
    "preprocessor = PoseEmotionPreprocessor()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPROCESSING TRAINING DATA\")\n",
    "print(\"=\"*70)\n",
    "train_frames, train_pose, train_emotion, train_labels_array = preprocess_and_cache_dataset(\n",
    "    train_paths, train_labels, 'train', preprocessor\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPROCESSING VALIDATION DATA\")\n",
    "print(\"=\"*70)\n",
    "val_frames, val_pose, val_emotion, val_labels_array = preprocess_and_cache_dataset(\n",
    "    val_paths, val_labels, 'val', preprocessor\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… PREPROCESSING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training samples: {len(train_frames)}\")\n",
    "print(f\"Validation samples: {len(val_frames)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ac28d",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Section 8: Build Multi-Modal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c615f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Attention Layer\n",
    "class AttentionLayer(layers.Layer):\n",
    "    def __init__(self, units=128, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], self.units),\n",
    "                                initializer='glorot_uniform', trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', shape=(self.units,),\n",
    "                                initializer='zeros', trainable=True)\n",
    "        self.u = self.add_weight(name='attention_context', shape=(self.units,),\n",
    "                                initializer='glorot_uniform', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        uit = tf.tanh(tf.tensordot(x, self.W, axes=1) + self.b)\n",
    "        ait = tf.tensordot(uit, self.u, axes=1)\n",
    "        attention_weights = tf.nn.softmax(ait, axis=1)\n",
    "        attention_weights = tf.expand_dims(attention_weights, -1)\n",
    "        weighted_input = x * attention_weights\n",
    "        return tf.reduce_sum(weighted_input, axis=1)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config_dict = super(AttentionLayer, self).get_config()\n",
    "        config_dict.update({'units': self.units})\n",
    "        return config_dict\n",
    "\n",
    "print(\"âœ… AttentionLayer defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a9c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multimodal_model():\n",
    "    # INPUT LAYERS\n",
    "    frame_input = layers.Input(shape=(config.SEQUENCE_LENGTH, config.IMG_HEIGHT, config.IMG_WIDTH, 3), name='frames')\n",
    "    pose_input = layers.Input(shape=(config.SEQUENCE_LENGTH, config.POSE_DIM), name='pose')\n",
    "    emotion_input = layers.Input(shape=(config.SEQUENCE_LENGTH, config.EMOTION_DIM), name='emotion')\n",
    "    \n",
    "    # RGB BRANCH\n",
    "    mobilenet = MobileNetV2(input_shape=(config.IMG_HEIGHT, config.IMG_WIDTH, 3),\n",
    "                            include_top=False, weights='imagenet', pooling='avg')\n",
    "    mobilenet.trainable = False\n",
    "    \n",
    "    x_rgb = layers.TimeDistributed(mobilenet, name='mobilenet')(frame_input)\n",
    "    x_rgb = layers.Dropout(0.3)(x_rgb)\n",
    "    x_rgb = layers.Bidirectional(layers.LSTM(config.LSTM_UNITS, return_sequences=True, \n",
    "                                              dropout=0.3, recurrent_dropout=0.2), name='rgb_bilstm')(x_rgb)\n",
    "    x_rgb = layers.Dropout(0.4)(x_rgb)\n",
    "    rgb_output = AttentionLayer(128, name='rgb_attention')(x_rgb)\n",
    "    rgb_output = layers.Dense(256, activation='relu')(rgb_output)\n",
    "    rgb_output = layers.BatchNormalization()(rgb_output)\n",
    "    rgb_output = layers.Dropout(0.5)(rgb_output)\n",
    "    \n",
    "    # POSE BRANCH\n",
    "    x_pose = layers.BatchNormalization()(pose_input)\n",
    "    x_pose = layers.TimeDistributed(layers.Dense(128, activation='relu'))(x_pose)\n",
    "    x_pose = layers.Dropout(0.3)(x_pose)\n",
    "    x_pose = layers.Bidirectional(layers.LSTM(128, return_sequences=True, \n",
    "                                               dropout=0.3, recurrent_dropout=0.2), name='pose_bilstm')(x_pose)\n",
    "    x_pose = layers.Dropout(0.4)(x_pose)\n",
    "    pose_output = AttentionLayer(64, name='pose_attention')(x_pose)\n",
    "    pose_output = layers.Dense(128, activation='relu')(pose_output)\n",
    "    pose_output = layers.BatchNormalization()(pose_output)\n",
    "    pose_output = layers.Dropout(0.5)(pose_output)\n",
    "    \n",
    "    # EMOTION BRANCH\n",
    "    x_emotion = layers.BatchNormalization()(emotion_input)\n",
    "    x_emotion = layers.TimeDistributed(layers.Dense(64, activation='relu'))(x_emotion)\n",
    "    x_emotion = layers.Dropout(0.3)(x_emotion)\n",
    "    x_emotion = layers.Bidirectional(layers.LSTM(64, return_sequences=True,\n",
    "                                                  dropout=0.3, recurrent_dropout=0.2), name='emotion_bilstm')(x_emotion)\n",
    "    x_emotion = layers.Dropout(0.4)(x_emotion)\n",
    "    emotion_output = layers.GlobalAveragePooling1D()(x_emotion)\n",
    "    emotion_output = layers.Dense(64, activation='relu')(emotion_output)\n",
    "    emotion_output = layers.BatchNormalization()(emotion_output)\n",
    "    emotion_output = layers.Dropout(0.5)(emotion_output)\n",
    "    \n",
    "    # ADAPTIVE FUSION\n",
    "    rgb_proj = layers.Dense(256, activation='relu', name='rgb_proj')(rgb_output)\n",
    "    pose_proj = layers.Dense(256, activation='relu', name='pose_proj')(pose_output)\n",
    "    emotion_proj = layers.Dense(256, activation='relu', name='emotion_proj')(emotion_output)\n",
    "    \n",
    "    stacked = tf.stack([rgb_proj, pose_proj, emotion_proj], axis=1, name='feature_stack')\n",
    "    fusion_weights = layers.Dense(3, activation='softmax', name='fusion_weights')(layers.Flatten()(stacked))\n",
    "    fusion_weights = layers.Reshape((3, 1))(fusion_weights)\n",
    "    weighted_features = layers.Multiply()([stacked, fusion_weights])\n",
    "    fused = layers.Lambda(lambda x: tf.reduce_sum(x, axis=1), name='fusion_sum')(weighted_features)\n",
    "    \n",
    "    # CLASSIFICATION HEAD\n",
    "    x = layers.Dense(512, activation='relu')(fused)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    output = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=[frame_input, pose_input, emotion_input], outputs=output,\n",
    "                  name='multimodal_violence_detector')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_multimodal_model()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(config.LEARNING_RATE),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(name='precision'),\n",
    "             keras.metrics.Recall(name='recall'), keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL BUILT & COMPILED\")\n",
    "print(\"=\"*70)\n",
    "model.summary()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272af9a7",
   "metadata": {},
   "source": [
    "## ðŸ‹ï¸ Section 9: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e75051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(config.MODEL_SAVE_DIR, 'best_multimodal_model.h5'),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=config.PATIENCE_EARLY_STOP,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=config.PATIENCE_REDUCE_LR,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Class weights\n",
    "unique, counts = np.unique(train_labels_array, return_counts=True)\n",
    "class_weights = {int(cls): len(train_labels_array) / (len(unique) * count) for cls, count in zip(unique, counts)}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfee5511",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸš€ STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training samples: {len(train_frames)}\")\n",
    "print(f\"Validation samples: {len(val_frames)}\")\n",
    "print(f\"Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"Epochs: {config.EPOCHS}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    x=[train_frames, train_pose, train_emotion],\n",
    "    y=train_labels_array,\n",
    "    validation_data=([val_frames, val_pose, val_emotion], val_labels_array),\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    epochs=config.EPOCHS,\n",
    "    callbacks=[checkpoint, early_stop, reduce_lr],\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "final_model_path = os.path.join(config.MODEL_SAVE_DIR, 'final_multimodal_model.h5')\n",
    "model.save(final_model_path)\n",
    "\n",
    "# Save history\n",
    "history_path = os.path.join(config.MODEL_SAVE_DIR, 'training_history.json')\n",
    "with open(history_path, 'w') as f:\n",
    "    history_dict = {key: [float(v) for v in values] for key, values in history.history.items()}\n",
    "    json.dump(history_dict, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… TRAINING COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best Validation Accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "print(f\"Best Validation AUC: {max(history.history['val_auc']):.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8224786",
   "metadata": {},
   "source": [
    "## ðŸ“Š Section 10: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac31e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].plot(history.history['accuracy'], label='Train')\n",
    "axes[0, 0].plot(history.history['val_accuracy'], label='Val')\n",
    "axes[0, 0].set_title('Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(history.history['loss'], label='Train')\n",
    "axes[0, 1].plot(history.history['val_loss'], label='Val')\n",
    "axes[0, 1].set_title('Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(history.history['precision'], label='Train')\n",
    "axes[1, 0].plot(history.history['val_precision'], label='Val')\n",
    "axes[1, 0].set_title('Precision', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(history.history['recall'], label='Train')\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Val')\n",
    "axes[1, 1].set_title('Recall', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.MODEL_SAVE_DIR, 'training_history.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred_proba = model.predict([val_frames, val_pose, val_emotion], batch_size=config.BATCH_SIZE)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "y_true = val_labels_array.astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_true, y_pred, target_names=['Non-Fight', 'Fight'], digits=4))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53028b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and ROC\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Non-Fight', 'Fight'], yticklabels=['Non-Fight', 'Fight'])\n",
    "axes[0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('True')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.4f})')\n",
    "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[1].set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.MODEL_SAVE_DIR, 'evaluation.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… ROC AUC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a75d0",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Complete! Model saved to `/kaggle/working/violence_detection_models/`\n",
    "\n",
    "### Files Saved:\n",
    "- `best_multimodal_model.h5` - Best model (use this!)\n",
    "- `final_multimodal_model.h5` - Final epoch model\n",
    "- `training_history.json` - Training metrics\n",
    "- `training_history.png` - Training plots\n",
    "- `evaluation.png` - Confusion matrix & ROC\n",
    "\n",
    "**Download from the Output tab on the right! â†’**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
